{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "696175c9-278e-4ec3-96ae-d819f44cf808",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T23:29:14.716049Z",
     "iopub.status.busy": "2023-04-26T23:29:14.716049Z",
     "iopub.status.idle": "2023-04-26T23:29:21.439055Z",
     "shell.execute_reply": "2023-04-26T23:29:21.438131Z",
     "shell.execute_reply.started": "2023-04-26T23:29:14.716049Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready: 2023-04-27 02:29:21\n"
     ]
    }
   ],
   "source": [
    "%run functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "151d11f4-9459-4a89-a26a-96ef295adfbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T23:29:21.441056Z",
     "iopub.status.busy": "2023-04-26T23:29:21.440056Z",
     "iopub.status.idle": "2023-04-26T23:29:28.963266Z",
     "shell.execute_reply": "2023-04-26T23:29:28.962279Z",
     "shell.execute_reply.started": "2023-04-26T23:29:21.441056Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark_Tutorial</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2520a36f130>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .appName('PySpark_Tutorial')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02c0b147-a6df-456f-991c-0c6c3477a5eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T21:47:01.550698Z",
     "iopub.status.busy": "2023-04-26T21:47:01.550698Z",
     "iopub.status.idle": "2023-04-26T21:47:01.561707Z",
     "shell.execute_reply": "2023-04-26T21:47:01.559753Z",
     "shell.execute_reply.started": "2023-04-26T21:47:01.550698Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_schema = StructType([StructField('user', StringType(), True), StructField('proj', StringType(), True), StructField('rubricName', StringType(), True), StructField('rubricID', IntegerType(), True), StructField('time', StringType(), True), StructField('date', StringType(), True), StructField('org', StringType(), True), StructField('branch', StringType(), True), StructField('prod', StringType(), True), StructField('tx', StringType(), True), StructField('fl', StringType(), True), StructField('devmod', StringType(), True), StructField('lat', StringType(), True), StructField('lon', StringType(), True)])\n",
    "df_list = [i for i in files_list_chain if 'Астана' in i or 'Бишкек' in i or 'Москва' in i or 'ОАЭ' in i or 'Ташкент' in i]\n",
    "df_list_new = [i for i in files_list_chain if i not in df_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c832331d-7eae-4d4e-80b9-512516dadf6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T21:47:08.822477Z",
     "iopub.status.busy": "2023-04-26T21:47:08.821468Z",
     "iopub.status.idle": "2023-04-26T22:54:30.152759Z",
     "shell.execute_reply": "2023-04-26T22:54:30.146211Z",
     "shell.execute_reply.started": "2023-04-26T21:47:08.822477Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 20/20 [1:07:21<00:00, 202.06s/it]\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm(df_list_new):\n",
    "    df = spark.read.option(\"header\",\"true\").csv(file, schema=df_schema)\n",
    "    df.createOrReplaceTempView('df')\n",
    "    city = file.split('\\\\')[2]\n",
    "    df_new = spark.sql(f'''\n",
    "    select \n",
    "    rubricName\n",
    "    , \"{city}\" city\n",
    "    , time\n",
    "    , year(time) year_r\n",
    "    , hour(time) hour_r\n",
    "    , SUBSTR(time, 6, 5) date_r\n",
    "    , case when weekofyear(time) > 9 then 1 else weekofyear(time) end week_r\n",
    "    , case when extract(dayofweek from time) = 1 then 7\n",
    "        else extract(dayofweek from time)-1\n",
    "    end week_day_r\n",
    "\n",
    "    from df\n",
    "    ''').where('year_r in (2020,2021,2022,2023) and month(time) < 3 ')\n",
    "\n",
    "    df_new.write.mode('append').parquet(f'files/ten_city/df_dates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aae03b93-f906-46cc-b2de-7e8432823432",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T22:55:50.381852Z",
     "iopub.status.busy": "2023-04-26T22:55:50.381852Z",
     "iopub.status.idle": "2023-04-26T22:57:01.738479Z",
     "shell.execute_reply": "2023-04-26T22:57:01.737500Z",
     "shell.execute_reply.started": "2023-04-26T22:55:50.381852Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "906098201"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(f'files/ten_city/df_dates').where('rubricName is not null').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ad5e347-dccc-4dc9-8a68-298b0f156148",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T23:22:10.717355Z",
     "iopub.status.busy": "2023-04-26T23:22:10.717355Z",
     "iopub.status.idle": "2023-04-26T23:22:13.445556Z",
     "shell.execute_reply": "2023-04-26T23:22:13.444421Z",
     "shell.execute_reply.started": "2023-04-26T23:22:10.717355Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1759325082"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(f'files/ten_city/df_dates').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbf0a8d7-2aba-4840-81ee-77bd74036b61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T23:29:28.965263Z",
     "iopub.status.busy": "2023-04-26T23:29:28.964265Z",
     "iopub.status.idle": "2023-04-26T23:29:35.203126Z",
     "shell.execute_reply": "2023-04-26T23:29:35.202123Z",
     "shell.execute_reply.started": "2023-04-26T23:29:28.965263Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dates = spark.read.parquet(f'files/ten_city/df_dates')\n",
    "df_dates.createOrReplaceTempView('df_dates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80ef9a6-c516-4097-8b5b-f46041f1130b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40f1772b-0316-401b-ba3b-d0184ab56a6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-26T23:29:40.115479Z",
     "iopub.status.busy": "2023-04-26T23:29:40.114480Z",
     "iopub.status.idle": "2023-04-26T23:53:56.349168Z",
     "shell.execute_reply": "2023-04-26T23:53:56.348179Z",
     "shell.execute_reply.started": "2023-04-26T23:29:40.115479Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [24:16<00:00, 364.05s/it]\n"
     ]
    }
   ],
   "source": [
    "for col_r in tqdm(['hour_r', 'date_r', 'week_r', 'week_day_r']):\n",
    "    df_col = spark.sql(f'''       \n",
    "    select \n",
    "        year_r\n",
    "        , rubricName\n",
    "        , city\n",
    "        , {col_r}\n",
    "        , count(*) cnt\n",
    "    from df_dates\n",
    "    group by\n",
    "        year_r\n",
    "        , rubricName\n",
    "        , city\n",
    "        , {col_r}\n",
    "    ''')\n",
    "\n",
    "    df_col.write.mode('overwrite').parquet(f'files/ten_city/df_dates_{col_r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "837d3241-4a65-4bdd-b3f8-1e062fc5ec41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T00:51:37.977871Z",
     "iopub.status.busy": "2023-04-27T00:51:37.977871Z",
     "iopub.status.idle": "2023-04-27T00:51:54.330919Z",
     "shell.execute_reply": "2023-04-27T00:51:54.329928Z",
     "shell.execute_reply.started": "2023-04-27T00:51:37.977871Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:16<00:00,  4.08s/it]\n"
     ]
    }
   ],
   "source": [
    "for col_r in tqdm(['hour_r', 'date_r', 'week_r', 'week_day_r']):\n",
    "    df_dates_col = spark.read.parquet(f'files/ten_city/df_dates_{col_r}')\n",
    "    df_dates_col.createOrReplaceTempView('df_dates_col')\n",
    "    df_norm = spark.sql(f'''       \n",
    "    select \n",
    "        year_r\n",
    "        , rubricName\n",
    "        , city\n",
    "        , {col_r}\n",
    "        , sum(cnt) over(partition by rubricName, {col_r}) / sum(cnt) over(partition by rubricName)*100 cnt\n",
    "    from df_dates_col\n",
    "    ''')\n",
    "\n",
    "    df_norm.write.mode('overwrite').parquet(f'files/ten_city/df_dates_norm_{col_r}')\n",
    "\n",
    "    df_piv = spark.read.parquet(f'files/ten_city/df_dates_norm_{col_r}')\n",
    "    df_piv_col_r = df_piv.groupBy(\"rubricName\").pivot(col_r).sum(\"cnt\")\n",
    "    df_piv_col_r.write.mode('overwrite').parquet(f'files/ten_city/df_dates_piv_{col_r}')\n",
    "\n",
    "    df_piv = spark.read.parquet(f'files/ten_city/df_dates_piv_{col_r}')\n",
    "    for i in [i for i in df_piv.columns if i!='rubricName']:\n",
    "        df_piv = df_piv.withColumnRenamed(i, 'c'+i.replace('-','_'))\n",
    "    df_piv.write.mode('overwrite').parquet(f'files/ten_city/df_dates_piv_norm_{col_r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "99cda9f7-09bc-4c54-a114-025f8ac701a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T00:48:45.886014Z",
     "iopub.status.busy": "2023-04-27T00:48:45.886014Z",
     "iopub.status.idle": "2023-04-27T00:48:46.006635Z",
     "shell.execute_reply": "2023-04-27T00:48:46.006635Z",
     "shell.execute_reply.started": "2023-04-27T00:48:45.886014Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "acd8a205-29dd-4c14-972b-babee8bfd2f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T00:57:01.715358Z",
     "iopub.status.busy": "2023-04-27T00:57:01.715358Z",
     "iopub.status.idle": "2023-04-27T00:57:01.806450Z",
     "shell.execute_reply": "2023-04-27T00:57:01.804453Z",
     "shell.execute_reply.started": "2023-04-27T00:57:01.715358Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mamed\\AppData\\Local\\Temp\\ipykernel_4424\\2621735795.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df_piv_mean = df_piv.toPandas().mean()\n"
     ]
    }
   ],
   "source": [
    "df_piv_mean = df_piv.toPandas().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "406dba48-bda6-4692-a39d-8b92473af0f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T01:00:21.991773Z",
     "iopub.status.busy": "2023-04-27T01:00:21.991773Z",
     "iopub.status.idle": "2023-04-27T01:00:22.081065Z",
     "shell.execute_reply": "2023-04-27T01:00:22.081065Z",
     "shell.execute_reply.started": "2023-04-27T01:00:21.991773Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_piv_pd = df_piv.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d62fbe72-245e-4a47-9a15-9bc8c3da0901",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T01:06:37.996887Z",
     "iopub.status.busy": "2023-04-27T01:06:37.996887Z",
     "iopub.status.idle": "2023-04-27T01:06:38.776594Z",
     "shell.execute_reply": "2023-04-27T01:06:38.776594Z",
     "shell.execute_reply.started": "2023-04-27T01:06:37.996887Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(df_piv_pd.shape[0]):\n",
    "    df_piv_pd.at[i, 'diff'] = abs(abs(df_piv_mean - df_piv_pd.iloc[i]).sum() / df_piv_mean.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "edb7e69f-c6c9-4bda-bf80-e868fa882dad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-27T01:13:11.616817Z",
     "iopub.status.busy": "2023-04-27T01:13:11.616817Z",
     "iopub.status.idle": "2023-04-27T01:13:11.635695Z",
     "shell.execute_reply": "2023-04-27T01:13:11.634696Z",
     "shell.execute_reply.started": "2023-04-27T01:13:11.616817Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "diff_01 = df_piv_pd[df_piv_pd['diff'] <= 0.1][['rubricName']]\n",
    "diff_02 = df_piv_pd[(df_piv_pd['diff'] > 0.1) & (df_piv_pd['diff'] <= 0.2)][['rubricName']]\n",
    "diff_03 = df_piv_pd[(df_piv_pd['diff'] > 0.2) & (df_piv_pd['diff'] <= 0.3)][['rubricName']]\n",
    "diff_04 = df_piv_pd[(df_piv_pd['diff'] > 0.3) & (df_piv_pd['diff'] <= 0.4)][['rubricName']]\n",
    "diff_05 = df_piv_pd[(df_piv_pd['diff'] > 0.4) & (df_piv_pd['diff'] <= 0.5)][['rubricName']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c33b6c-7647-443a-b008-b82958de0c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b32935-fa67-4bf0-a418-81144c66a05a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f06f4c-ed40-4931-9a38-ae2b25194912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_dates(col, df_new='df_new'):\n",
    "    dff1 = spark.sql(f'''\n",
    "\n",
    "    with t as\n",
    "        (\n",
    "        select \n",
    "            year_r\n",
    "            , rubricName\n",
    "            , city\n",
    "            , {col}\n",
    "            , count(*) cnt\n",
    "        from {df_new}\n",
    "        group by\n",
    "            year_r\n",
    "            , rubricName\n",
    "            , city\n",
    "            , {col}\n",
    "        order by\n",
    "            year_r\n",
    "            , rubricName\n",
    "            , city\n",
    "            , {col}\n",
    "        )\n",
    "    ,gr as\n",
    "        (\n",
    "        select\n",
    "            rubricName \n",
    "            ,{col} \n",
    "            ,max(case when city = 'Астана'  then cnt end) cnt_a\n",
    "            ,max(case when city = 'Бишкек'  then cnt end) cnt_b\n",
    "            ,max(case when city = 'Москва'  then cnt end) cnt_m\n",
    "            ,max(case when city = 'ОАЭ'     then cnt end) cnt_o\n",
    "            ,max(case when city = 'Ташкент' then cnt end) cnt_t\n",
    "        from t\n",
    "        group by\n",
    "            rubricName\n",
    "            ,{col}\n",
    "        order by\n",
    "            rubricName\n",
    "            ,{col}    \n",
    "        )\n",
    "\n",
    "    select\n",
    "        rubricName \n",
    "        ,{col}\n",
    "        ,sum(cnt_a) over(partition by rubricName, {col}) / sum(cnt_a) over(partition by rubricName)*100 cnt_a\n",
    "        ,sum(cnt_b) over(partition by rubricName, {col}) / sum(cnt_b) over(partition by rubricName)*100 cnt_b\n",
    "        ,sum(cnt_m) over(partition by rubricName, {col}) / sum(cnt_m) over(partition by rubricName)*100 cnt_m\n",
    "        ,sum(cnt_o) over(partition by rubricName, {col}) / sum(cnt_o) over(partition by rubricName)*100 cnt_o\n",
    "        ,sum(cnt_t) over(partition by rubricName, {col}) / sum(cnt_t) over(partition by rubricName)*100 cnt_t\n",
    "        \n",
    "    from gr\n",
    "\n",
    "    ''').toPandas()\n",
    "    return dff1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b0222c-ac1e-4b22-853b-598ad3155685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dates_file(name):\n",
    "    df = spark.read.parquet(f'files/df_filtered_{name}')\n",
    "    df.createOrReplaceTempView('df')\n",
    "\n",
    "    df_new = spark.sql('''\n",
    "\n",
    "    select \n",
    "    rubricName\n",
    "    , time\n",
    "    , year(time) year_r\n",
    "    , hour(time) hour_r\n",
    "    , SUBSTR(time, 6, 5) date_r\n",
    "    , case when weekofyear(time) > 9 then 1 else weekofyear(time) end week_r\n",
    "    , case when extract(dayofweek from time) = 1 then 7\n",
    "        else extract(dayofweek from time)-1\n",
    "    end week_day_r\n",
    "\n",
    "    from df\n",
    "    ''')\n",
    "\n",
    "    df_new.write.mode('overwrite').parquet(f'files/df_dates_{name}')\n",
    "\n",
    "    df_new = spark.read.parquet(f'files/df_dates_{name}').where('year_r in (2020,2021,2022,2023) and month(time) < 3 ')\n",
    "    df_new.createOrReplaceTempView('df_new')\n",
    "\n",
    "    dff1 = count_dates('hour_r')\n",
    "    dff2 = count_dates('date_r')\n",
    "    dff3 = count_dates('week_r')\n",
    "    dff4 = count_dates('week_day_r')\n",
    "\n",
    "    t = ['Рубрика','Час','Количество запросов 2020','Количество запросов 2021','Количество запросов 2022','Количество запросов 2023',\n",
    "        'max_2020','max_2021','max_2022','max_2023','min_2020','min_2021','min_2022','min_2023','up_2021','up_2022','up_2023','down_2021','down_2022','down_2023', 'delta_score']\n",
    "\n",
    "\n",
    "    t[1] = 'Час'\n",
    "    dff1.columns = t\n",
    "    t[1] = 'Дата'\n",
    "    dff2.columns = t\n",
    "    t[1] = 'Неделя'\n",
    "    dff3.columns = t\n",
    "    t[1] = 'День недели'\n",
    "    dff4.columns = t\n",
    "\n",
    "    with pd.ExcelWriter(f'files/df_dates_{name}.xlsx') as writer:\n",
    "        dff1.to_excel(writer, sheet_name='Часы', index=False)\n",
    "        dff2.to_excel(writer, sheet_name='Даты', index=False)\n",
    "        dff3.to_excel(writer, sheet_name='Недели', index=False)\n",
    "        dff4.to_excel(writer, sheet_name='Дни недели', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
